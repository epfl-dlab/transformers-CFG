Subject: [PATCH] error refactor
---
Index: tests/_tokenizer_common.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/_tokenizer_common.py b/tests/_tokenizer_common.py
--- a/tests/_tokenizer_common.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/tests/_tokenizer_common.py	(date 1707474468902)
@@ -97,7 +97,7 @@
         #
         # start_rule_id = parsed_grammar.symbol_table["root"]
         #
-        # recognizer = GrammarRecognizer(parsed_grammar.grammar_encoding, start_rule_id)
+        # recognizer = GrammarRecognizer.init_from_src(parsed_grammar.grammar_encoding, start_rule_id)
         #
         # self.assertTrue(recognizer._accept_string(emoji, recognizer.stacks))

Index: transformers_cfg/recognizer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/transformers_cfg/recognizer.py b/transformers_cfg/recognizer.py
--- a/transformers_cfg/recognizer.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/transformers_cfg/recognizer.py	(date 1707476820625)
@@ -12,22 +12,37 @@


 class GrammarRecognizer:
-    def __init__(self, grammar_encoding: List[int], start_rule_id: int):
-        # strictly speaking, we don't need to copy grammar_encoding because we don't modify it
-        # but we do it anyway to be safe
-        # in case where the grammar is very large, we can consider not copying it
-        self.grammar_encoding = copy.deepcopy(grammar_encoding)
-        self.rule_offsets: List[int] = self.init_rules(start_rule_id)
-        # each stack is a list of indices into grammar_encoding
-        # each index points to a rule's
-        self.stacks: List[List[int]] = self.init_stack(start_rule_id)
+    # def __init__(self, grammar_encoding: List[int], start_rule_id: int):
+    #     # strictly speaking, we don't need to copy grammar_encoding because we don't modify it
+    #     # but we do it anyway to be safe
+    #     # in case where the grammar is very large, we can consider not copying it
+    #     self.grammar_encoding = copy.deepcopy(grammar_encoding)
+    #     self.rule_offsets: List[int] = self.init_rules(start_rule_id)
+    #     # each stack is a list of indices into grammar_encoding
+    #     # each index points to a rule's
+    #     self.stacks: List[List[int]] = self.init_stack(start_rule_id)
+
+    def __init__(self, grammar_encoding, rule_offsets: List[int], stacks: List[List[int]]):
+        self.grammar_encoding = grammar_encoding
+        self.rule_offsets = rule_offsets
+        self.stacks = stacks

-    def init_rules(self, start_rule_id: int) -> List[int]:
+    @classmethod
+    def init_from_src(cls, grammar_encoding: List[int], start_rule_id: int):
+        rule_offsets = cls.init_rules(grammar_encoding, start_rule_id)
+        import pdb; pdb.set_trace()
+        # Create an instance of cls
+        instance = cls(rule_offsets, [[]])
+        instance.stacks = instance.init_stack(start_rule_id)
+        return instance
+
+    @staticmethod
+    def init_rules(grammar_encoding: List[int], start_rule_id: int) -> List[int]:
         _rule_offset = 0
         rule_offsets = []
         # Build `rules` as an array of rule IDs to their positions in `grammar_src`
-        while self.grammar_encoding[_rule_offset] != 0xFFFF:
-            rule_id = self.grammar_encoding[_rule_offset]
+        while grammar_encoding[_rule_offset] != 0xFFFF:
+            rule_id = grammar_encoding[_rule_offset]
             # store the offset idx
             if len(rule_offsets) <= rule_id:
                 rule_offsets.extend([-1] * (rule_id - len(rule_offsets) + 1))
@@ -38,16 +53,16 @@
             simple_rhs_offset = _rule_offset + 1

             # Skip rule alternates
-            while self.grammar_encoding[simple_rhs_offset] != END_OF_RULE_MARKER:
+            while grammar_encoding[simple_rhs_offset] != END_OF_RULE_MARKER:
                 simple_rhs_offset = (
-                    simple_rhs_offset + 1 + self.grammar_encoding[simple_rhs_offset]
+                    simple_rhs_offset + 1 + grammar_encoding[simple_rhs_offset]
                 )

             # Skip 0 denoting end of rule
             # _rule_offset += 1
             _rule_offset = simple_rhs_offset + 1

-        retrieved_start_rule_id = self.grammar_encoding[rule_offsets[start_rule_id]]
+        retrieved_start_rule_id = grammar_encoding[rule_offsets[start_rule_id]]
         assert retrieved_start_rule_id == start_rule_id

         return rule_offsets
@@ -56,7 +71,7 @@

         stacks = []
         # Loop over alternates of start rule to build initial stacks
-        sub_rhs_offset = self.rule_offsets[start_rule_id] + 1
+        sub_rhs_offset = self.grammar_encoding[start_rule_id] + 1
         while self.grammar_encoding[sub_rhs_offset]:
             stack: List[int] = []
             # If alternate is nonempty, add to stack
@@ -83,7 +98,7 @@
         else:
             ref_rule_id = self.grammar_encoding[cur_element_offset + 1]
             # find the offset of the referenced rule
-            ref_subrule_offset = self.rule_offsets[ref_rule_id] + 1
+            ref_subrule_offset = self.grammar_encoding[ref_rule_id] + 1
             new_stacks: List[List[int]] = []
             # Loop over alternates of referenced rule to build new stacks
             while self.grammar_encoding[ref_subrule_offset] != END_OF_RULE_MARKER:
@@ -214,7 +229,7 @@
     print(f"symbol_ids: \n{parsed_grammar.symbol_table}")

     start_rule_id = parsed_grammar.symbol_table["root"]
-    recognizer = GrammarRecognizer(parsed_grammar.grammar_encoding, start_rule_id)
+    recognizer = GrammarRecognizer.init_from_src(parsed_grammar.grammar_encoding, start_rule_id)
     res = recognizer._accept_string("12222", recognizer.stacks)
     print(f"12222: {res}")
     res = recognizer._accept_string("12222+", recognizer.stacks)
Index: tests/test_json_arr.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_json_arr.py b/tests/test_json_arr.py
--- a/tests/test_json_arr.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/tests/test_json_arr.py	(date 1707474468922)
@@ -28,7 +28,7 @@

         start_rule_id = parsed_grammar.symbol_table["root"]

-        recognizer = GrammarRecognizer(parsed_grammar.grammar_encoding, start_rule_id)
+        recognizer = GrammarRecognizer.init_from_src(parsed_grammar.grammar_encoding, start_rule_id)

         for json in jsons:
             self.assertEqual(
Index: transformers_cfg/token_grammar_recognizer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/transformers_cfg/token_grammar_recognizer.py b/transformers_cfg/token_grammar_recognizer.py
--- a/transformers_cfg/token_grammar_recognizer.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/transformers_cfg/token_grammar_recognizer.py	(date 1707474468914)
@@ -66,7 +66,7 @@
         assert len(self.mapping) == len(
             self.token_trie
         ), f"{len(self.mapping)}, {len(self.token_trie)}"
-        self.grammar = GrammarRecognizer(grammar_encoding, self.start_rule_id)
+        self.grammar = GrammarRecognizer.init_from_src(grammar_encoding, self.start_rule_id)

     def _consume_token_id(self, token_id: int, stacks: List[List[int]]):
         if self.grammar._must_stop(stacks):
Index: tests/test_unicode.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_unicode.py b/tests/test_unicode.py
--- a/tests/test_unicode.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/tests/test_unicode.py	(date 1707474468926)
@@ -24,7 +24,7 @@

         start_rule_id = parsed_grammar.symbol_table["root"]

-        recognizer = GrammarRecognizer(parsed_grammar.grammar_encoding, start_rule_id)
+        recognizer = GrammarRecognizer.init_from_src(parsed_grammar.grammar_encoding, start_rule_id)

         self.assertTrue(recognizer._accept_string(japanese, recognizer.stacks))

@@ -40,6 +40,6 @@

         start_rule_id = parsed_grammar.symbol_table["root"]

-        recognizer = GrammarRecognizer(parsed_grammar.grammar_encoding, start_rule_id)
+        recognizer = GrammarRecognizer.init_from_src(parsed_grammar.grammar_encoding, start_rule_id)

         self.assertTrue(recognizer._accept_string(emoji, recognizer.stacks))
Index: tests/test_json.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_json.py b/tests/test_json.py
--- a/tests/test_json.py	(revision 3b964653167363f8a58533a5fa9ac91a55361696)
+++ b/tests/test_json.py	(date 1707474468922)
@@ -42,7 +42,7 @@

         start_rule_id = parsed_grammar.symbol_table["root"]

-        self.recognizer = GrammarRecognizer(
+        self.recognizer = GrammarRecognizer.init_from_src(
             parsed_grammar.grammar_encoding, start_rule_id
         )
